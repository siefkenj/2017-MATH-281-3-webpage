\documentclass[letter]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage{tikz}

%%%
% Set up the margins to use a fairly large area of the page
%%%
\oddsidemargin=.2in
\evensidemargin=.2in
\textwidth=6in
\topmargin=-.4in
\textheight=9.0in
\parskip=.07in
\parindent=0in
\pagestyle{fancy}

%%%
% Set up the header
%%%
\newcommand{\setheader}[6]{
	\lhead{{\sc #1}\\{\sc #2}}
	\rhead{
		{\bf #3} 
		\ifthenelse{\equal{#4}{}}{}{(#4)}\\
		{\bf #5} 
		\ifthenelse{\equal{#6}{}}{}{(#6)}%
	}
}

%%%
% Set up some shortcut commands
%%%
\newcommand{\R}{\mathbb{R}}
\renewcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Proj}{\mathrm{proj}}
\newcommand{\Perp}{\mathrm{perp}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\Span}{\mathrm{span}}
\newcommand{\Null}{\mathrm{null}}
\newcommand{\Det}{\mathrm{det}}
\newcommand{\Rank}{\mathrm{rank}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\d}{\mathrm{d}}

%%%
% This is where the body of the document goes
%%%
\begin{document}
\setheader{Math 281-3}{Homework 7}{Due Friday, May 26}{}{}{}

	\begin{enumerate}
		\item Let $A=\mat{1&2&1\\1&1&1\\1&0&0}$ and $B=\mat{1&2&1\\1&1&1\\1&0&x}$.
		\begin{enumerate}
			\item Compute $\Det(A)$.
			\item Compute $\Det(B)$.  For what values of $x$ is $B$ not invertible?
		\end{enumerate}
		
		\item Let $A=\mat{1&2\\5&9}$.
		\begin{enumerate}
			\item Find an equation for the function $p(x)=\Det(A-xI)$ (this is called the
				\emph{characteristic polynomial} of $A$).
			\item For what values of $x$ is $A-xI$ non-invertible?
			\item Compute $p(A)$, the polynomial $p$ with the matrix $A$ plugged into it.  When you plug a matrix
				into a polynomial, replace any constant terms $k$ with the matrix $kI$.
				Can you guess
				why $p$ is called an \emph{annihilating} polynomial for $A$?
		\end{enumerate}


		\item \emph{You may use Matlab/Octave to assist with the calculations for this problem.}  Let
			\[
				\vec b_1=\mat{1\\1\\1}\qquad\vec b_2=\mat{1\\-1\\0}\qquad
				\vec b_3=\mat{-1\\0\\1}\qquad \vec c=\mat{1\\2\\3}
			\]
			and let $\mathcal B=\{\vec b_1,\vec b_2,\vec b_3\}$ and $\mathcal S=\{\vec e_1,\vec e_2,\vec e_3\}$.
			Suppose $T:\R^3\to\R^3$ is a linear transformation and $T(\vec b_1)=2\vec b_1$, 
			$T(\vec b_2)=3\vec b_2$, and $T(\vec b_3)=-\vec b_3$.
			\begin{enumerate}
				\item Compute $[\vec c]_{\mathcal B}$.
				\item Compute $[T\vec c]_{\mathcal B}$ and $[T\vec c]_{\mathcal S}$.
				\item Find a matrix for $T$ in the $\mathcal B$ basis (i.e., 
					the matrix $[T]_{\mathcal B}$) and a matrix for $T$ in
					the $\mathcal S$ basis (i.e., $[T]_{\mathcal S}$).
				\item Compute det$([T]_{\mathcal S})$ and det$([T]_{\mathcal B})$.  Why do you get the same number?
			\end{enumerate}
		
		\item {\sc Playing with coordinate systems} Let
			\[
				\vec u=\mat{1\\0\\0}\qquad\vec v=\mat{0\\1\\1}\qquad \mathcal B=\{\vec u,\vec v\}\qquad \mathcal P=\Span\,\mathcal B.
			\]
			\begin{enumerate}
				\item 
				For any point $\vec x\in\mathcal P$, $\vec x=\alpha\vec u+\beta\vec v$ for some $\alpha,\beta$.
				So, we might say $[\vec x]_{\mathcal B} =\mat{\alpha\\\beta}$.  Let $\|\cdot\|_{\mathcal B}:\R^2\to\R$
				be the \emph{induced norm} on $\R^2$ defined by,
				\[
					\|[\vec x]_{\mathcal B}\|_{\mathcal B} = \|\vec x\|.
				\]
				(Remember, $[\vec x]_{\mathcal B}$ is a list of two numbers---it isn't the same thing as the vector $\vec x$).
				Write down a formula for $\left\|\mat{\alpha\\\beta}\right\|_{\mathcal B}$.

				\item Just like an induced norm, we can also have an induced dot-product.  Let the
					inclusion map $\iota:\R^2\to\mathcal P$
					be defined as
					\[
						\iota\mat{\alpha\\\beta} = \alpha \vec u+\beta\vec v.
					\]
					Define the $\mathcal B$-dot-product, $\odot$, as 
					\[
						\vec a\odot \vec b = (\iota \vec a)\cdot (\iota \vec b)
					\]
					for any $\vec a,\vec b\in\R^2$.  Verify that $\|\vec a\|_{\mathcal B} = \sqrt{\vec a\odot\vec a}$
					for $\vec a\in\R^2$ and draw the set of all unit vectors in $\R^2$ under the norm $\|\cdot\|_{\mathcal B}$.
				\item For $\vec a,\vec b\in\R^2$, we will \emph{define} the angle between $\vec a$ and $\vec b$
					to be the number $\theta$ so that $\vec a\odot\vec b=\|\vec a\|_{\mathcal B}\|\vec b\|_{\mathcal B}\cos\theta$.
					Let $\vec c_1=\mat{1\\0}$, $\vec c_2=\mat{2\\1}$, $\vec c_3=\mat{1\\1}$.  For each $\vec c_i$, draw
					the set of all vectors orthogonal to $\vec c_i$ with respect to $\odot$.  Is the notion of
					angle coming from $\odot$ the same as from the standard dot product?  Is it always different?


				\item  An \emph{inner product} on a vector space $V$ is a function $\langle \cdot,\cdot\rangle:V\times V\to\R$
					that is symmetric, bilinear, and positive definite.  That is, for any $\vec u,\vec v,\vec w\in V$
					and $\alpha\in\R$,
					\begin{enumerate}
						\item $\langle \vec u,\vec v\rangle =\langle \vec v,\vec u\rangle$ (symmetric)
						\item $\langle \alpha\vec u,\vec v\rangle =\alpha\langle \vec u,\vec v\rangle =\langle \vec u,\alpha\vec v\rangle$\\
							and $\langle \vec u+\vec w,\vec v\rangle =\langle \vec u,\vec v\rangle+\langle \vec w,\vec v\rangle$\\
							and $\langle \vec u,\vec v+\vec w\rangle =\langle \vec u,\vec v\rangle+\langle \vec u,\vec w\rangle$ (bilinear)
						\item $\langle \vec u,\vec u\rangle\geq 0$ and $\langle \vec u,\vec u\rangle=0$ if and only if $\vec u=0$ (positive definite).
					\end{enumerate}

					Show that both the standard dot product and $\odot$ are inner products on $\R^2$.
				\item Notice that for $\vec x,\vec y\in\R^2$, $\vec x\cdot\vec y = \vec x^T\vec y=\vec x^TI\vec y$.
					Find a matrix $A$ such that $\vec x\odot \vec y=\vec x^TA\vec y$.  Matrices like $A$ 
					show up in the study of relativity.  When you are moving at relativistic speeds, the angle
					you perceive between two objects is different than someone at rest would perceive.  Using
					a matrix like $A$ or an inner product like $\odot$ allows you compensate for how relativistic
					effects change your perceptions.
			\end{enumerate}

		\item Let $A=\mat{7.5&-7&0\\3.5&-3&0\\7&-7&0.5}$.
		\begin{enumerate}
			\item Find all eigenvalues of $A$ and their multiplicity.
			\item Find a basis for each eigenspace of $A$.
			\item Is $A$ diagonalizable?  If so, diagonalize $A$.  If not, explain why
				not.
			\item Compute $\lim_{n\to\infty} A^n$ or explain why it doesn't exist.  
			\item $X$ is a matrix with eigenvalues $2,3,3$.  Does $\lim_{n\to\infty} X^n$ exist?
				Does it depend on whether $X$ is diagonalizable?
		\end{enumerate}
		
		\item 

		McDonalds recently negotiated a large purchasing deal for fish, chicken, and beef.  They have agreed to purchase
		40 million tons of fish, 40 million tons of chicken, and 100 million tons of beef.  As such, they want to create an advertising campaign
		to ensure that consumers eat the correct portion of each meat product.

		After paying to have a commercial produced, McDonalds collects the following data: After watching the commercial once,
		a person who initially wanted fish now has a 10\% chance of buying a fish product, a 60\% chance of buying a beef product,
		and a 30\% chance of buying a chicken product; after watching, a person who initially wanted to buy a beef product has
		a 20\% chance of buying a fish product, a 60\% chance of buying a beef product, and a 20\% chance of buying a chicken product;
		after watching, a person who initially wanted to by a chicken product has a 40\% chance of buying a fish product, a 50\%
		chance of buying a beef product, and a 10\% chance of buying a chicken product.
		\begin{enumerate}
			\item If the vector $\vec e_1$ represents a person who wants 
			to buy a fish product, $\vec e_2$ represents a person who wants 
			to buy a beef product, and $\vec e_3$ represents a person who wants 
			to buy a chicken product, find a matrix $M$ such that $M\vec e_i$ gives the probability of buying fish, beef,
			or chicken after watching the commercial once.

			\item Compute the eigenvalues and eigenvectors of $M$.

			\item Assume that a fish product takes 50 grams
			of fish, a beef product takes 50 grams of beef,
			and a chicken product takes 50 grams of chicken.
			Further, assume that each time a person watches
			the commercial it has the same impact (i.e.,
			watching the commercial twice means the likelihood
			of buying a particular product is given by $M^2$).
			If McDonalds ensures that the average customer
			sees the commercial 3000 times, what are the
			relative proportions of fish, beef, and chicken
			McDonalds expects to sell?

			\item Should McDonalds run the ad? Does the
			initial population's preferences for fish,
			beef, or chicken matter? \emph{Explain your
			reasoning.}
		\end{enumerate}

		\item
			We are often interested in bounding error.  Suppose a measurement from
			a one-dimensional linear process is $7\pm 0.1$.  That is our margin of
			error is $\pm 0.1$.  If we amplify the input by a factor of two, 
			we should report $14\pm 0.2$, with the margin of error doubled.

			This one-dimensional example is straightforward.
			However, things are more complicated when we deal with multiple dimensions.
			In higher dimensions, your error takes the form of an error vector, and your 
			transformation may be a matrix.

			Let $\vec m_a$ be the actual value of a quantity.  We say that $\vec m$ is
			a measurement of $\vec m_a$ within tolerance $\epsilon$ if 
			$\|\vec m_a-\vec m\|\leq \epsilon$.  Another way of saying that is
			\[
				\vec m_a = \vec m + \vec v\qquad\text{where} \qquad \|\vec v\|<\epsilon.
			\]
			Here, $\vec v$ is our \emph{error vector}.
			If we apply linear
			transformation $T$, we have that $T\vec m_a = T\vec m + T\vec v$ and so
			our error goes from $\vec v$ to $T\vec v$.  The question becomes, can we find a number
			$k$ so that if $\|\vec v\|<\epsilon$, then $\|T\vec v\|<k\epsilon$?  The smallest
			such number
			is called the \emph{norm} of the matrix $T$, and we write $\|T\|=k$.

			\begin{enumerate}
				\item Explain why $\|T\|=\max \{\|T\vec u\|:\vec u\text{ is a unit vector}\}$.
				\item Let $A=\mat{1&2\\3&1}$.  Use Matlab/octave to 
					create a $2\times 360$ matrix $C$ whose column vectors
					are uniformly distributed points on the unit circle (Hint:
					a command like {\tt [cos(1:10)]} will produce the vector $\mat{\cos 1&\cos 2&\cdots &\cos 10}$).
					
					Graph $C$ and $AC$ on the same plot.  Notice that $C$ 
					should be a circle (you may need to rescale your axes with {\tt xlim([-4 4]); ylim([-4 4])}
					to get $C$ to look like a circle) and $AC$ should be an ellipse.  
					Explain how the major and minor axes of the ellipse $AC$ relate to $\|A\|$.

				\item Numerically estimate $\|A\|$. 
					If $\vec v$ is an error vector with $\|\vec v\|=0.01$, give a theoretical
					upper bound on $\|A^{30}\vec v\|$.  (Do this without using Matlab to compute $A^{30}$.)
				\item Suppose $B$ is a diagonalizable $2\times 2$ matrix with eigenvalues
					$\lambda_1$ and $\lambda_2$ satisfying $|\lambda_1|,|\lambda_2|\leq 1$.
					Come up with a conjecture for what an upper bound for $\|B\|$ might be.
					Then, numerically experiment.  Try to explain your findings.  (Hint:
					if you want a ``random'' matrix with particular eigenvalues,
					you might consider something like {\tt r=rand(2); B=r*D*r\textasciicircum(-1)}
					for a well-chosen {\tt D}).  Make sure to be good scientists and
					seek for evidence to \emph{disprove} your hypothesis. 
			\end{enumerate}
		\item {\sc Let's blow the lid off}.  It's time to tiptoe into the realm of infinite
			dimensions.  Using the idea of \emph{sampling}, we can easily turn a function
					into a finite dimensional vector.  Let $f:[0,1]\to\R$ be a continuous
					function, and let $s_n:\{\text{continuous functions}\}\to\R^n$
					be the function that creates a vector out of a function by sampling
					that function at $n$ equally spaced points.  That is,
					\[
						s_n(f) = \mat{f(0)\\[2pt]f(\frac{1}{n})\\[2pt]f(\frac{2}{n})\\\vdots\\f(\frac{n-1}{n})}.
					\]
			\begin{enumerate}
				\item Using $s_n$, we can now carry over many of our familiar operations from $\R^n$ to 
					the space of functions.  Let $\mathcal P_n$ be the set of polynomials of degree at most $n$.
					We can attempt to induce an \emph{inner product} on $\mathcal P_n$ as follows,
					\[
						\langle\cdot,\cdot\rangle_n:\{\text{continuous functions}\}\to \R\qquad\text{ defined by }\qquad
						\langle p,q\rangle_n = s_n(p)\cdot s_n(q).
					\]
					Show that $\langle \cdot,\cdot\rangle_n$ induces an inner product on $\mathcal P_{n-1}$ but
					not on $\mathcal P_n$ (Hint: a degree $n$ polynomial is uniquely determined by $n+1$ points).
				\item Recall that with an inner product $\langle\cdot,\cdot\rangle_n$, we define an induced norm
					via $\|f\|_{\langle\cdot,\cdot\rangle_n}=\sqrt{\langle f, f\rangle_n}$.  Our goal
					is to find an induced norm that works for all polynomials simultaneously.  One idea might
					be to try to take the limit of $\langle\cdot,\cdot\rangle_n$, however, this runs into problems,
					because $\lim_{n\to\infty} \langle 1,1\rangle_n=\infty$.  Let's fix this by \emph{normalizing}.

					Define
					\[
						\langle a,b\rangle_{\lim} = \lim_{n\to\infty} \frac{1}{n}\langle a,b\rangle_n
					\]
					and let $\|\cdot\|_{\lim}$ be the induced norm.  Compute $\|f\|_{\lim}$ 
					for $f(x)=1$, $f(x)=x$, and $f(x)=x^2$.  (Hint: you already know how to do this!  Think
					about Riemann sums!)
				\item It turns out $\langle \cdot,\cdot\rangle_{\lim}$ is an inner product on all continuous
					functions on $[0,1]$.  Thus, we can use it to do projections and find angles just
					like we would in $\R^n$.  

					Let $\mathcal B=\{1,x,x^2,x^3\}$ be a basis for $\mathcal P_3$.  Apply the Gram-Schmidt process
					to $\mathcal B$ (read your textbook or Wikipedia if we haven't done it in class yet) to
					find an orthonormal basis for $\mathcal P_3$.
				\item Use your basis from the previous part to find $\proj_{\mathcal P_3} \tfrac{1}{1+10x^2}$.  Plot
					$\tfrac{1}{1+10x^2}$ and your projection on the same axes.  Is it a good approximation?  How
					does it compare with the degree-3 Taylor approximation? (You may use a computer
					to estimate the coefficients).

				\item {\sc The Fourier Basis}.  The Fourier basis is one of the most important bases for
					the space of functions.  Let $\mathcal B_n=\{1,\sin 2\pi t,\cos 2\pi t,\sin 4\pi t,\cos 4\pi t,\sin 6\pi t,\cos 6\pi t,
					\ldots, \sin 2n\pi t,\cos 2n\pi t\}$.  Apply the Gram-Schmidt process to $\mathcal B_n$
					to obtain the orthonormal Fourier basis $\mathcal F_n$.
				\item Let $D$ be differentiation.  Is the Fourier basis an eigenbasis for $D$?  How
					about for the second derivative $D^2$? Write out 
					$[D]_{\mathcal F_3}$ and $[D^2]_{\mathcal F_3}$.
				\item Using complex numbers, the Fourier basis can be written in terms of exponentials
					as $\mathcal F_n=\{ e^{2k\pi i t}: 0\leq k\leq n\}$.  Show that each element in
					the complex Fourier basis is an eigenvector of differentiation and give its
					corresponding eigenvalue.  Then marvel at how projecting a function onto the complex
					Fourier basis looks a lot like taking the Laplace transform (it isn't the same, it's
					of course called the \emph{Fourier} transform, but it's darn close).
			\end{enumerate}

		\item {\sc An Ultra-violet Catastrophe}.  We can use the Fourier basis to quantify the Heisenberg uncertainty
			principle!  And, because I'm sure you're feeling computational, we'll do so numerically with Matlab/Octave.
			\begin{enumerate}
				\item Create a $129\times 129$ matrix $F$ whose columns are the basis elements in $\mathcal F_{64}$
					evaluated at the points $\mathbf x=(0/129,1/129,2/129,\ldots,128/129)$.  If you plot the first three columns of
					$F$ with the command {\tt plot(F(:,1:3))} you should see the graphs of $y=1$, $y=\sqrt{2}\sin 2\pi t$
					and $y=\sqrt{2}\cos 2\pi t$.  If you'd like to see $F$ in all its glory, you can use
					the command {\tt imshow(F)}.

					Compute $F^TF$ and explain your result.
				\item Since $F$ changes basis from the Fourier basis to the standard basis, $F^{-1}$ is almost $F^T$ and goes
					from the standard basis to the Fourier basis.  (In your computations, use {\tt F'} instead of
					{\tt F\textasciicircum(-1)} for numerical stability reasons.)  Create four column vectors
					\[
					\vec v_0=3\sin(4\pi \mathbf x)\qquad \vec v_{0.2}=3\sin(4\pi (\mathbf x-0.2))\]\[
						\vec v_{0.5}=3\sin(4\pi (\mathbf x-0.5))\qquad\vec v_{0.7}=3\sin(4\pi (\mathbf x-0.7))
					\]
					where $\mathbf x$ ranges over the values specified in part 1.  Use $F^{-1}$ to write
					these vectors in the Fourier basis.  What frequencies in the Fourier basis have non-zero coefficients?
					Does this make sense?
				\item Recall that $a\sin t+b\cos t$ is a shifted sine wave with amplitude $\sqrt{a^2+b^2}$.  Using just the 
					coefficients from the Fourier basis found in part 2, verify that $\vec v_i$ is a shifted sine wave
					with amplitude 3 for each $i\in\{0,.2,.5,.7\}$.
				\item According to Einstein, the energy contained in a wave of frequency $k$ is proportional to $ka$
					where $a$ is the amplitude of the wave.
					Since we're doing math, we will assume the energy of a wave of frequency $k$ and amplitude $a$
					is \emph{equal} to $ka$.  Suppose a particle is described in the Fourier basis by
					\[
						[\vec p]_{\mathcal F_{64}}=\mat{1&1&1.5&2&0&0&\cdots & 0}.
					\]
					Compute the energy of $\vec p$.
				\item We are now ready to see the Heisenberg uncertainty principle in action.  Using
					the command {\tt s=4; xs=(0:128)/129; s*normpdf(s*(xs-0.5))} we can create
					a bell curve centered at $0.5$ with standard deviation $1/s$.  As {\tt s}
					increases, the bell curve will become more and more concentrated.  Using your
					knowledge of the Fourier basis, compute
					the wave-energy contained in bell curves with standard deviations $1/4,1/40, 1/400, 1/4000$.
					If a particle has a finite amount of energy, is there a limit to how concentrated in
					one spot it can be?
				\item The command {\tt a=.3; b=.4; xs=(0:128)/129; 1/(b-a)*(xs > a).*(xs < b) } will
					create the distribution of a particle that is contained in the range $[a,b]$ with
					100\% certainty.  Compute the energy of such a particle.  Does changing the width of 
					the interval affect the amount of energy in the particle?  How does the energy
					contained in this particle compare to that in a bell-curve-distributed particle?
				\item Let $\vec p$ be a particle strictly contained in $[a,b]$.  We can
					attempt to see what it would look like if this particle existed in nature
					by forcing its energy to be low.  That is, if we compute $F^{-1}\vec p$, we can
					then use the use a truncation function to set the coefficients of all high-frequency
					sine and cosine waves to zero.  Let $t:\{\text{ vectors in the basis }\mathcal F_{64}\}\to
					\{\text{ vectors in the basis }\mathcal F_{64}\}$ defined so that $t(\vec v)$
					sets the coefficients of every sine and cosine wave with frequency higher than 25 to zero
					and leaves the rest of the coefficients alone.

					Now, we might say that $F(t(F^{-1}\vec p))$ is what $\vec p$ would ``look like''
					if we forced it to have finite energy.  For $[a,b]=[.3,.4]$ and $[a,b]=[.4,.6]$
					graph a picture of what $\vec p$ would ``look like.''
			\end{enumerate}


	\end{enumerate}
\end{document}
